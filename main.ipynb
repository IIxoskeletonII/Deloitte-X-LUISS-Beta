{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# DiscoverAI: Review-Aware Semantic Search\n\nThis notebook runs the complete DiscoverAI pipeline on Google Colab.\n\n**Before running:**\n1. Upload `Health_and_Personal_Care.jsonl.gz` and `meta_Health_and_Personal_Care.jsonl.gz` to `src/io/input/raw/`\n2. Make sure the Colab runtime is set to **GPU** (Runtime > Change runtime type > T4 GPU)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. Clone Repository and Setup Environment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nREPO_URL = \"https://github.com/IIxoskeletonII/Deloitte-X-LUISS-Beta.git\"\nREPO_DIR = \"/content/Deloitte-X-LUISS-Beta\"\n\nif not os.path.exists(REPO_DIR):\n    !git clone {REPO_URL} {REPO_DIR}\n    print(f\"Cloned repo to {REPO_DIR}\")\nelse:\n    print(f\"Repo already exists at {REPO_DIR}\")\n\nos.chdir(REPO_DIR)\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q pandas==2.1.4 numpy==1.26.2 pyarrow==14.0.2 sentence-transformers==2.2.2 transformers==4.36.2 faiss-cpu==1.7.2 gradio==4.12.0 tqdm==4.66.1\n\nimport subprocess\nresult = subprocess.run([\"pip\", \"install\", \"-q\", \"faiss-gpu-cu12\"], capture_output=True)\nif result.returncode == 0:\n    print(\"FAISS-GPU installed.\")\nelse:\n    print(\"FAISS-GPU not available, using CPU version.\")\n\nimport torch\nimport platform\n\nprint(f\"\\nPython: {platform.python_version()}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 0.1 Upload Dataset Files\nRun this cell, then use the file picker to upload both `.jsonl.gz` files. They will be placed in the correct directory automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom pathlib import Path\nfrom google.colab import files\n\nRAW_DIR = Path(\"src/io/input/raw\")\nRAW_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"Upload both .jsonl.gz files now:\")\nuploaded = files.upload()\n\nfor filename, content in uploaded.items():\n    dest = RAW_DIR / filename\n    with open(dest, \"wb\") as f:\n        f.write(content)\n    print(f\"  Saved: {dest} ({len(content) / 1e6:.1f} MB)\")\n\nprint(\"\\nFiles in raw directory:\")\nfor f in RAW_DIR.iterdir():\n    print(f\"  {f.name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Extraction\n",
    "Decompress the raw `.jsonl.gz` files into readable JSONL format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_extraction import main as run_extraction\n",
    "run_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "Validate column names, check missing values, and confirm join keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eda_preliminary import peek_data\n",
    "\n",
    "print(\">>> ANALYZING METADATA...\")\n",
    "meta_df = peek_data(\"meta_Health_and_Personal_Care.jsonl\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(\">>> ANALYZING REVIEWS...\")\n",
    "reviews_df = peek_data(\"Health_and_Personal_Care.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "Clean metadata, filter low-engagement reviews, merge datasets, rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import main as run_preprocessing\n",
    "run_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Product-Level Semantic Embeddings\n",
    "Encode product metadata and reviews with `all-mpnet-base-v2`, fuse into product vectors, build FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modelling import main as run_modelling\n",
    "run_modelling()\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Review Summarization\n",
    "Generate human-readable summaries per product using `facebook/bart-large-cnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.summarization import main as run_summarization\n",
    "run_summarization()\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Named Entity Recognition\n",
    "Extract brand names, organizations, and other entities from reviews using `dslim/bert-base-NER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.entity_recognition import main as run_entity_recognition\n",
    "run_entity_recognition()\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Search Engine Validation\n",
    "Load all artifacts and run sample queries to verify the system works end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.search_engine import main as run_search_test\n",
    "run_search_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 7.1 Quality Report\nGenerates a human-readable quality report with pipeline statistics, sample summaries, sample entities, and sample search results. Saves to `quality_report.txt` for offline review.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom src.search_engine import SearchEngine\n\nOUTPUT_DIR = Path(\"src/io/output\")\nREPORT_PATH = \"quality_report.txt\"\n\nengine = SearchEngine()\nlines = []\n\nlines.append(\"=\" * 70)\nlines.append(\"DISCOVERAI - QUALITY REPORT\")\nlines.append(\"=\" * 70)\n\nlines.append(\"\\n--- PIPELINE STATISTICS ---\")\n\ndf_processed = pd.read_parquet(OUTPUT_DIR / \"processed_data.parquet\")\nlines.append(f\"Processed dataset shape: {df_processed.shape}\")\nlines.append(f\"Columns: {list(df_processed.columns)}\")\nlines.append(f\"Unique products: {df_processed['parent_asin'].nunique()}\")\nlines.append(f\"Total reviews: {len(df_processed)}\")\n\nembeddings = np.load(OUTPUT_DIR / \"product_embeddings.npy\")\nlines.append(f\"\\nEmbedding matrix shape: {embeddings.shape}\")\nlines.append(f\"Any NaN in embeddings: {np.any(np.isnan(embeddings))}\")\nlines.append(f\"FAISS index size: {engine.index.ntotal}\")\n\ndf_summaries = pd.read_parquet(OUTPUT_DIR / \"product_summaries.parquet\")\nlines.append(f\"\\nProducts summarized: {len(df_summaries)}\")\nlines.append(f\"Non-empty summaries: {(df_summaries['summary'].str.len() > 0).sum()}\")\n\ndf_entities = pd.read_parquet(OUTPUT_DIR / \"product_entities.parquet\")\nnon_empty_ents = sum(1 for e in df_entities[\"entities\"] if e != \"{}\")\nlines.append(f\"Products with entities: {non_empty_ents} / {len(df_entities)}\")\n\nlines.append(\"\\n\\n--- SAMPLE SUMMARIES (5 random products) ---\")\nsamples = df_summaries[df_summaries[\"summary\"].str.len() > 0].sample(min(5, len(df_summaries)), random_state=42)\nfor _, row in samples.iterrows():\n    meta = engine.metadata_lookup.get(row[\"parent_asin\"], {})\n    title = meta.get(\"product_title\", \"Unknown\")\n    lines.append(f\"\\nProduct: {title}\")\n    lines.append(f\"ASIN: {row['parent_asin']}\")\n    lines.append(f\"Reviews used: {row['num_reviews_used']}\")\n    lines.append(f\"Summary: {row['summary']}\")\n\nlines.append(\"\\n\\n--- SAMPLE ENTITIES (5 products with most entities) ---\")\nentity_counts = []\nfor _, row in df_entities.iterrows():\n    try:\n        ents = json.loads(row[\"entities\"])\n        entity_counts.append((row[\"parent_asin\"], ents, len(ents)))\n    except (json.JSONDecodeError, TypeError):\n        continue\nentity_counts.sort(key=lambda x: x[2], reverse=True)\nfor asin, ents, count in entity_counts[:5]:\n    meta = engine.metadata_lookup.get(asin, {})\n    title = meta.get(\"product_title\", \"Unknown\")\n    lines.append(f\"\\nProduct: {title}\")\n    lines.append(f\"ASIN: {asin}\")\n    lines.append(f\"Entities ({count}): {json.dumps(ents, indent=2)}\")\n\nlines.append(\"\\n\\n--- SEARCH QUALITY TEST ---\")\ntest_queries = [\n    \"low-priced skincare product\",\n    \"organic shampoo for sensitive scalp\",\n    \"vitamins for energy and focus\",\n    \"toothpaste for whitening\",\n    \"moisturizer for dry skin\",\n]\nfor query in test_queries:\n    lines.append(f\"\\nQuery: '{query}'\")\n    results = engine.search(query, top_k=5)\n    if not results:\n        lines.append(\"  No results found.\")\n        continue\n    for i, r in enumerate(results, 1):\n        lines.append(f\"  {i}. [score={r['score']:.3f}] {r['product_title']}\")\n        lines.append(f\"     Rating: {r['average_rating']} | Store: {r['store']}\")\n        if r[\"summary\"]:\n            lines.append(f\"     Summary: {r['summary'][:150]}...\")\n\nlines.append(\"\\n\\n--- RECOMMENDATION TEST ---\")\ntest_asin = df_summaries.iloc[0][\"parent_asin\"]\nmeta = engine.metadata_lookup.get(test_asin, {})\nlines.append(f\"Source product: {meta.get('product_title', 'Unknown')} ({test_asin})\")\nrecs = engine.recommend(test_asin, top_k=5)\nfor i, r in enumerate(recs, 1):\n    lines.append(f\"  {i}. [score={r['score']:.3f}] {r['product_title']}\")\n\nlines.append(\"\\n\" + \"=\" * 70)\nlines.append(\"END OF REPORT\")\nlines.append(\"=\" * 70)\n\nreport_text = \"\\n\".join(lines)\n\nwith open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(report_text)\n\nprint(report_text)\nprint(f\"\\nReport saved to: {REPORT_PATH}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Demo\n",
    "Launch the Gradio interface with semantic search and product explorer tabs.\n",
    "A public URL will be generated for sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.demo import create_demo\n",
    "\n",
    "app = create_demo()\n",
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}